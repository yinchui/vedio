# MVP Phase 3: AIåŠŸèƒ½é›†æˆ Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** é›†æˆé€šä¹‰åƒé—®å’Œè®¯é£æ˜Ÿç«APIï¼Œå®ç°AIè§†é¢‘åˆ†æã€æ™ºèƒ½å‰ªè¾‘å»ºè®®ã€è‡ªåŠ¨å­—å¹•ç”Ÿæˆå’ŒéŸ³ä¹åŒ¹é…åŠŸèƒ½ã€‚

**Architecture:** åç«¯é›†æˆAI SDKï¼Œå®ç°å¼‚æ­¥ä»»åŠ¡å¤„ç†ã€‚å‰ç«¯å®ç°AIåˆ†æUIã€è¿›åº¦æ˜¾ç¤ºå’Œç»“æœå±•ç¤ºã€‚

**Tech Stack:** é€šä¹‰åƒé—®SDK (dashscope), è®¯é£æ˜Ÿç«SDK, Celery (ä»»åŠ¡é˜Ÿåˆ—), WebSocket (è¿›åº¦æ¨é€), librosa (éŸ³é¢‘åˆ†æ)

**Duration:** 3å‘¨ï¼ˆWeek 7-9ï¼‰

**Prerequisites:**
- Phase 1-2å·²å®Œæˆ
- é€šä¹‰åƒé—®API Keyå·²é…ç½®
- è®¯é£æ˜Ÿç«API Keyå·²é…ç½®

---

## AI SDKé›†æˆ

### Task 1: é€šä¹‰åƒé—®è§†é¢‘ç†è§£é›†æˆ

**Files:**
- Create: `backend/services/ai/qwen_service.py`
- Create: `backend/services/ai/__init__.py`
- Create: `backend/requirements.txt` (æ›´æ–°)

**Step 1: å®‰è£…é€šä¹‰åƒé—®SDK**

æ›´æ–° `backend/requirements.txt`:

```
dashscope==1.14.0
```

```bash
cd backend
pip install dashscope
```

**Step 2: åˆ›å»ºé€šä¹‰åƒé—®æœåŠ¡**

åœ¨ `backend/services/ai/__init__.py`:

```python
# AI services package
```

åœ¨ `backend/services/ai/qwen_service.py`:

```python
import dashscope
from dashscope import MultiModalConversation
from pathlib import Path
from typing import List, Dict, Any
import base64
import logging

from config import QWEN_API_KEY, QWEN_MODEL

logger = logging.getLogger(__name__)

class QwenService:
    """é€šä¹‰åƒé—®è§†é¢‘ç†è§£æœåŠ¡"""

    def __init__(self, api_key: str = None):
        self.api_key = api_key or QWEN_API_KEY
        dashscope.api_key = self.api_key

    def analyze_video_frames(
        self,
        frame_paths: List[str],
        target_duration: int = None
    ) -> Dict[str, Any]:
        """
        åˆ†æè§†é¢‘å…³é”®å¸§

        Args:
            frame_paths: å…³é”®å¸§å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            target_duration: ç›®æ ‡å‰ªè¾‘æ—¶é•¿ï¼ˆç§’ï¼‰

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # æ„å»ºæ¶ˆæ¯
        messages = self._build_analysis_prompt(frame_paths, target_duration)

        try:
            response = MultiModalConversation.call(
                model=QWEN_MODEL,
                messages=messages
            )

            if response.status_code == 200:
                result = response.output.choices[0].message.content
                logger.info(f"è§†é¢‘åˆ†ææˆåŠŸ: {len(frame_paths)}å¸§")
                return self._parse_analysis_result(result)
            else:
                logger.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥: {response.message}")
                raise RuntimeError(f"APIè°ƒç”¨å¤±è´¥: {response.message}")

        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†æå¼‚å¸¸: {str(e)}")
            raise

    def _build_analysis_prompt(
        self,
        frame_paths: List[str],
        target_duration: int = None
    ) -> List[Dict]:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        # å°†å›¾ç‰‡è½¬ä¸ºbase64
        image_contents = []
        for path in frame_paths[:10]:  # æœ€å¤š10å¼ å›¾ç‰‡
            with open(path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
                image_contents.append({
                    "image": f"data:image/jpeg;base64,{image_data}"
                })

        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚æˆ‘ç»™ä½ ä¸€æ®µè§†é¢‘çš„å…³é”®å¸§å›¾åƒåºåˆ—ï¼Œè¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š

1. **åœºæ™¯æè¿°**ï¼šè¿™æ®µè§†é¢‘ä¸»è¦å±•ç¤ºäº†ä»€ä¹ˆåœºæ™¯ï¼Ÿï¼ˆå¦‚"æµ·è¾¹æ—¥è½"ã€"äººç‰©ç‰¹å†™"ã€"åŸå¸‚è¡—æ™¯"ç­‰ï¼‰

2. **æƒ…ç»ªæ°›å›´**ï¼šæ•´ä½“æƒ…ç»ªå’Œæ°›å›´æ˜¯ä»€ä¹ˆï¼Ÿç”¨æ ‡ç­¾è¡¨ç¤ºï¼ˆå¦‚happy, relaxed, excited, heartwarming, sadç­‰ï¼‰

3. **ç²¾å½©åº¦è¯„åˆ†**ï¼šè¿™æ®µå†…å®¹çš„å¯çœ‹æ€§å¦‚ä½•ï¼Ÿè¯„åˆ†0-100ï¼Œè€ƒè™‘ï¼š
   - ç”»é¢ç¾æ„Ÿ
   - åŠ¨ä½œä¸°å¯Œåº¦
   - æƒ…ç»ªé¥±æ»¡åº¦
   - å†…å®¹å¸å¼•åŠ›

4. **å‰ªè¾‘å»ºè®®**ï¼šæ˜¯å¦å»ºè®®ä¿ç•™è¿™æ®µå†…å®¹ï¼Ÿç»™å‡ºç†ç”±ã€‚

{"ç›®æ ‡ï¼šå‰ªè¾‘æˆ" + str(target_duration // 60) + "åˆ†é’Ÿå·¦å³çš„ç”Ÿæ´»ç±»çŸ­è§†é¢‘ã€‚" if target_duration else ""}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "scene_description": "åœºæ™¯æè¿°",
  "emotions": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "excitement_score": 85,
  "suggest_keep": true,
  "reason": "å»ºè®®ä¿ç•™æˆ–åˆ é™¤çš„ç†ç”±"
}}"""

        messages = [
            {
                "role": "user",
                "content": image_contents + [{"text": prompt}]
            }
        ]

        return messages

    def _parse_analysis_result(self, result: str) -> Dict[str, Any]:
        """è§£æAIè¿”å›ç»“æœ"""
        import json
        import re

        # å°è¯•æå–JSON
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤å€¼
        logger.warning(f"æ— æ³•è§£æAIè¿”å›ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼: {result}")
        return {
            "scene_description": "æœªçŸ¥åœºæ™¯",
            "emotions": ["neutral"],
            "excitement_score": 50,
            "suggest_keep": True,
            "reason": "åˆ†æç»“æœè§£æå¤±è´¥"
        }

    def generate_narrative_subtitle(
        self,
        scene_description: str,
        emotion: str,
        trigger: str
    ) -> str:
        """
        ç”Ÿæˆåœºæ™¯æè¿°å­—å¹•

        Args:
            scene_description: åœºæ™¯æè¿°
            emotion: æƒ…ç»ªæ ‡ç­¾
            trigger: è§¦å‘æ—¶æœº (scene_change/emotion_peak)

        Returns:
            å­—å¹•æ–‡æœ¬
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å­—å¹•æ’°å†™ä¸“å®¶ã€‚

åœºæ™¯ï¼š{scene_description}
æƒ…ç»ªï¼š{emotion}
æ—¶æœºï¼š{"åœºæ™¯è½¬æ¢" if trigger == "scene_change" else "æƒ…ç»ªé«˜ç‚¹"}

è¯·ç”Ÿæˆä¸€å¥ç®€çŸ­ã€æœ‰è¶£çš„æ—ç™½æ€§å­—å¹•ï¼ˆ10-20å­—ï¼‰ï¼Œç±»ä¼¼æŠ–éŸ³é£æ ¼ã€‚

ç¤ºä¾‹ï¼š
- "æ­¤æ—¶çš„æˆ‘è¿˜ä¸çŸ¥é“æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆ..."
- "ååœºé¢æ¥äº†"
- "è¿™ä¸€åˆ»ï¼Œæ—¶é—´ä»¿ä½›é™æ­¢äº†"
- "æ²¡æƒ³åˆ°å§"

åªè¿”å›å­—å¹•æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        messages = [{"role": "user", "content": prompt}]

        try:
            response = MultiModalConversation.call(
                model=QWEN_MODEL,
                messages=messages
            )

            if response.status_code == 200:
                text = response.output.choices[0].message.content.strip()
                # å»é™¤å¼•å·
                text = text.strip('"\'ã€Œã€')
                return text
            else:
                return "ç²¾å½©æ—¶åˆ»"

        except Exception as e:
            logger.error(f"ç”Ÿæˆå­—å¹•å¤±è´¥: {str(e)}")
            return "ç²¾å½©æ—¶åˆ»"

    def generate_keyword_tag(
        self,
        scene_description: str,
        emotion: str
    ) -> str:
        """
        ç”Ÿæˆå…³é”®è¯æ ‡æ³¨

        Args:
            scene_description: åœºæ™¯æè¿°
            emotion: æƒ…ç»ªæ ‡ç­¾

        Returns:
            å…³é”®è¯ï¼ˆå¦‚"éœ‡æ’¼"ã€"ç¬‘æ­»"ï¼‰
        """
        emotion_keywords = {
            "funny": ["ç¬‘æ­»", "å“ˆå“ˆ", "ç»äº†"],
            "shocking": ["éœ‡æ’¼", "ç»äº†", "ç‰›"],
            "heartwarming": ["â¤ï¸", "æ¸©é¦¨", "æš–"],
            "exciting": ["ç‡ƒ", "ğŸ”¥", "ç»"],
            "beautiful": ["ç¾", "ç»ç¾", "âœ¨"]
        }

        keywords = emotion_keywords.get(emotion, ["ç²¾å½©"])
        return keywords[0]
```

**Step 3: åˆ›å»ºæµ‹è¯•**

åœ¨ `backend/tests/test_qwen_service.py`:

```python
import pytest
from services.ai.qwen_service import QwenService

@pytest.fixture
def qwen_service():
    return QwenService()

def test_qwen_service_initialization(qwen_service):
    """æµ‹è¯•æœåŠ¡åˆå§‹åŒ–"""
    assert qwen_service.api_key is not None

@pytest.mark.skip("éœ€è¦çœŸå®çš„API Keyå’Œå›¾ç‰‡")
def test_analyze_video_frames(qwen_service):
    """æµ‹è¯•è§†é¢‘å¸§åˆ†æ"""
    frame_paths = ["path/to/frame1.jpg"]
    result = qwen_service.analyze_video_frames(frame_paths)

    assert "scene_description" in result
    assert "excitement_score" in result
```

**Step 4: æäº¤é€šä¹‰åƒé—®é›†æˆ**

```bash
git add backend/services/ai/ backend/requirements.txt backend/tests/
git commit -m "feat(backend): integrate Qwen AI for video analysis

- Install dashscope SDK
- Create QwenService for video frame analysis
- Implement narrative subtitle generation
- Add keyword tag generation
- Parse AI responses to structured data

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 2: è®¯é£æ˜Ÿç«è¯­éŸ³è¯†åˆ«é›†æˆ

**Files:**
- Create: `backend/services/ai/xunfei_service.py`
- Update: `backend/requirements.txt`

**Step 1: å®‰è£…è®¯é£SDK**

æ›´æ–° `backend/requirements.txt`:

```
websocket-client==1.6.0
```

```bash
pip install websocket-client
```

**Step 2: åˆ›å»ºè®¯é£æ˜Ÿç«æœåŠ¡**

åœ¨ `backend/services/ai/xunfei_service.py`:

```python
import hashlib
import hmac
import base64
import json
from datetime import datetime
from time import mktime
from wsgiref.handlers import format_date_time
from urllib.parse import urlencode, urlparse
import websocket
from pathlib import Path
from typing import List, Dict
import logging

from config import XUNFEI_APPID, XUNFEI_API_KEY, XUNFEI_API_SECRET

logger = logging.getLogger(__name__)

class XunfeiService:
    """è®¯é£æ˜Ÿç«è¯­éŸ³è¯†åˆ«æœåŠ¡"""

    def __init__(self):
        self.appid = XUNFEI_APPID
        self.api_key = XUNFEI_API_KEY
        self.api_secret = XUNFEI_API_SECRET
        self.host = "rtasr.xfyun.cn"
        self.request_line = "GET /v1/ws HTTP/1.1"

    def transcribe_audio(self, audio_path: Path) -> List[Dict[str, any]]:
        """
        è¯­éŸ³è½¬æ–‡å­—

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«æ—¶é—´æˆ³çš„æ–‡å­—åˆ—è¡¨
            [
                {"start_time": 0.5, "end_time": 2.3, "text": "ä½ å¥½"},
                ...
            ]
        """
        if not audio_path.exists():
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

        # ç®€åŒ–ç‰ˆå®ç°ï¼šä½¿ç”¨HTTP APIï¼ˆå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨WebSocketå®æ—¶è¯†åˆ«ï¼‰
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        logger.warning("è®¯é£æ˜Ÿç«é›†æˆä¸ºæ¨¡æ‹Ÿå®ç°ï¼Œè¯·é…ç½®çœŸå®API")

        return self._simulate_transcription(audio_path)

    def _simulate_transcription(self, audio_path: Path) -> List[Dict[str, any]]:
        """æ¨¡æ‹Ÿè¯­éŸ³è¯†åˆ«ï¼ˆå¼€å‘ç”¨ï¼‰"""
        # è¿”å›æ¨¡æ‹Ÿçš„è½¬å½•ç»“æœ
        return [
            {
                "start_time": 0.0,
                "end_time": 2.5,
                "text": "ä»Šå¤©å¤©æ°”çœŸä¸é”™",
                "words": [
                    {"text": "ä»Šå¤©", "start_time": 0.0, "duration": 0.5},
                    {"text": "å¤©æ°”", "start_time": 0.5, "duration": 0.5},
                    {"text": "çœŸ", "start_time": 1.0, "duration": 0.3},
                    {"text": "ä¸é”™", "start_time": 1.3, "duration": 0.7}
                ]
            },
            {
                "start_time": 3.0,
                "end_time": 5.5,
                "text": "é€‚åˆå‡ºå»èµ°èµ°",
                "words": [
                    {"text": "é€‚åˆ", "start_time": 3.0, "duration": 0.5},
                    {"text": "å‡ºå»", "start_time": 3.5, "duration": 0.5},
                    {"text": "èµ°èµ°", "start_time": 4.0, "duration": 0.8}
                ]
            }
        ]

    # TODO: å®ç°çœŸå®çš„WebSocketè¿æ¥å’Œå®æ—¶è¯†åˆ«
    # def _create_auth_url(self) -> str:
    #     """åˆ›å»ºé‰´æƒURL"""
    #     pass

    # def _on_message(self, ws, message):
    #     """WebSocketæ¶ˆæ¯å›è°ƒ"""
    #     pass
```

**Step 3: æäº¤è®¯é£æ˜Ÿç«é›†æˆ**

```bash
git add backend/services/ai/xunfei_service.py backend/requirements.txt
git commit -m "feat(backend): integrate Xunfei ASR service

- Create XunfeiService for speech recognition
- Add audio transcription method with timestamps
- Implement simulated transcription for development
- TODO: Real WebSocket implementation

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## è§†é¢‘åˆ†ææœåŠ¡

### Task 3: å®Œæ•´çš„è§†é¢‘åˆ†ææµç¨‹

**Files:**
- Create: `backend/services/video_analysis_service.py`
- Modify: `backend/services/video_service.py`

**Step 1: æ‰©å±•è§†é¢‘æœåŠ¡ - å…³é”®å¸§æå–**

åœ¨ `backend/services/video_service.py` ä¸­æ·»åŠ :

```python
def extract_key_frames(
    self,
    video_path: Path,
    output_dir: Path,
    interval: int = 10,
    max_frames: int = 30
) -> List[str]:
    """
    æå–å…³é”®å¸§ï¼ˆç”¨äºAIåˆ†æï¼‰

    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        max_frames: æœ€å¤§å¸§æ•°

    Returns:
        å…³é”®å¸§è·¯å¾„åˆ—è¡¨
    """
    from config import FFMPEG_PATH

    if not video_path.exists():
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    output_dir.mkdir(parents=True, exist_ok=True)

    # è·å–è§†é¢‘æ—¶é•¿
    info = self.extract_video_info(video_path)
    duration = info["duration"]

    # è®¡ç®—å®é™…å¸§æ•°
    actual_interval = max(interval, duration / max_frames)
    frame_count = min(int(duration / actual_interval), max_frames)

    frames = []

    for i in range(frame_count):
        timestamp = i * actual_interval
        output_file = output_dir / f"frame_{i:03d}.jpg"

        cmd = [
            FFMPEG_PATH,
            "-ss", str(timestamp),
            "-i", str(video_path),
            "-vframes", "1",
            "-q:v", "2",
            "-y",
            str(output_file)
        ]

        try:
            subprocess.run(cmd, capture_output=True, check=True)
            frames.append(str(output_file))
        except subprocess.CalledProcessError as e:
            logger.error(f"æå–å…³é”®å¸§å¤±è´¥: {e.stderr}")

    return frames

def extract_audio(self, video_path: Path, output_path: Path) -> str:
    """
    æå–éŸ³é¢‘

    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„

    Returns:
        éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    """
    from config import FFMPEG_PATH

    if not video_path.exists():
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    output_path.parent.mkdir(parents=True, exist_ok=True)

    cmd = [
        FFMPEG_PATH,
        "-i", str(video_path),
        "-vn",  # ä¸å¤„ç†è§†é¢‘
        "-acodec", "pcm_s16le",  # éŸ³é¢‘ç¼–ç 
        "-ar", "16000",  # é‡‡æ ·ç‡
        "-ac", "1",  # å•å£°é“
        "-y",
        str(output_path)
    ]

    try:
        subprocess.run(cmd, capture_output=True, check=True)
        logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {output_path}")
        return str(output_path)
    except subprocess.CalledProcessError as e:
        logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e.stderr}")
        raise RuntimeError(f"éŸ³é¢‘æå–å¤±è´¥: {e.stderr}")
```

**Step 2: åˆ›å»ºè§†é¢‘åˆ†ææœåŠ¡**

åœ¨ `backend/services/video_analysis_service.py`:

```python
from pathlib import Path
from typing import Dict, List, Any
import uuid
import json
import logging

from services.video_service import VideoService
from services.ai.qwen_service import QwenService
from services.ai.xunfei_service import XunfeiService
from config import CACHE_DIR

logger = logging.getLogger(__name__)

class VideoAnalysisService:
    """è§†é¢‘åˆ†ææœåŠ¡ - æ•´åˆAIåˆ†ææµç¨‹"""

    def __init__(self):
        self.video_service = VideoService()
        self.qwen_service = QwenService()
        self.xunfei_service = XunfeiService()

    def analyze_video(
        self,
        video_path: str,
        target_duration: int = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è§†é¢‘åˆ†ææµç¨‹

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            target_duration: ç›®æ ‡å‰ªè¾‘æ—¶é•¿ï¼ˆç§’ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(stage, progress)

        Returns:
            åˆ†æç»“æœ
        """
        video_path = Path(video_path)
        analysis_id = str(uuid.uuid4())
        work_dir = Path(CACHE_DIR) / analysis_id

        try:
            # Step 1: æå–å…³é”®å¸§ (0-30%)
            if progress_callback:
                progress_callback("æå–å…³é”®å¸§", 0)

            frames = self.video_service.extract_key_frames(
                video_path,
                work_dir / "frames",
                interval=10,
                max_frames=30
            )

            if progress_callback:
                progress_callback("æå–å…³é”®å¸§", 30)

            # Step 2: AIè§†é¢‘ç†è§£ (30-60%)
            if progress_callback:
                progress_callback("AIå†…å®¹ç†è§£", 30)

            ai_result = self.qwen_service.analyze_video_frames(
                frames,
                target_duration
            )

            if progress_callback:
                progress_callback("AIå†…å®¹ç†è§£", 60)

            # Step 3: æå–éŸ³é¢‘å¹¶è¯†åˆ« (60-90%)
            transcript = []
            video_info = self.video_service.extract_video_info(video_path)

            if video_info.get("has_audio"):
                if progress_callback:
                    progress_callback("è¯­éŸ³è¯†åˆ«", 60)

                audio_path = self.video_service.extract_audio(
                    video_path,
                    work_dir / "audio.wav"
                )

                transcript = self.xunfei_service.transcribe_audio(
                    Path(audio_path)
                )

            if progress_callback:
                progress_callback("è¯­éŸ³è¯†åˆ«", 90)

            # Step 4: ç”Ÿæˆå‰ªè¾‘å»ºè®® (90-100%)
            if progress_callback:
                progress_callback("ç”Ÿæˆå‰ªè¾‘å»ºè®®", 90)

            # æ„å»ºåˆ†æç»“æœ
            result = {
                "video_path": str(video_path),
                "duration": video_info["duration"],
                "scene_description": ai_result.get("scene_description", ""),
                "emotions": ai_result.get("emotions", []),
                "excitement_score": ai_result.get("excitement_score", 50),
                "suggest_keep": ai_result.get("suggest_keep", True),
                "reason": ai_result.get("reason", ""),
                "transcript": transcript,
                "frames": frames
            }

            if progress_callback:
                progress_callback("åˆ†æå®Œæˆ", 100)

            logger.info(f"è§†é¢‘åˆ†æå®Œæˆ: {video_path.name}")
            return result

        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            raise

    def generate_edit_suggestions(
        self,
        analysis_results: List[Dict],
        target_duration: int
    ) -> List[Dict]:
        """
        æ ¹æ®åˆ†æç»“æœç”Ÿæˆå‰ªè¾‘å»ºè®®

        Args:
            analysis_results: å¤šä¸ªè§†é¢‘çš„åˆ†æç»“æœ
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰

        Returns:
            å‰ªè¾‘å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        # æ”¶é›†æ‰€æœ‰å»ºè®®ä¿ç•™çš„ç‰‡æ®µ
        for result in analysis_results:
            if result["suggest_keep"]:
                suggestions.append({
                    "video_path": result["video_path"],
                    "start_time": 0,  # ç®€åŒ–ï¼šä½¿ç”¨æ•´æ®µ
                    "end_time": result["duration"],
                    "score": result["excitement_score"],
                    "reason": result["reason"]
                })

        # æŒ‰è¯„åˆ†æ’åº
        suggestions.sort(key=lambda x: x["score"], reverse=True)

        # é€‰æ‹©ç‰‡æ®µç›´åˆ°è¾¾åˆ°ç›®æ ‡æ—¶é•¿
        selected = []
        total_duration = 0

        for suggestion in suggestions:
            duration = suggestion["end_time"] - suggestion["start_time"]
            if total_duration + duration <= target_duration:
                selected.append(suggestion)
                total_duration += duration
            elif total_duration < target_duration * 0.8:
                # è£å‰ªæœ€åä¸€ä¸ªç‰‡æ®µ
                remaining = target_duration - total_duration
                suggestion["end_time"] = suggestion["start_time"] + remaining
                selected.append(suggestion)
                break

        return selected
```

**Step 3: åˆ›å»ºAPIç«¯ç‚¹**

åˆ›å»º `backend/api/routes/analysis.py`:

```python
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import List
import uuid

from models.database import get_db
from models.asset import Asset
from services.video_analysis_service import VideoAnalysisService
import json

router = APIRouter(prefix="/api/analysis", tags=["analysis"])
analysis_service = VideoAnalysisService()

# å­˜å‚¨ä»»åŠ¡çŠ¶æ€ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨Redisï¼‰
analysis_tasks = {}

class StartAnalysisRequest(BaseModel):
    asset_ids: List[str]
    target_duration: int  # ç§’
    aspect_ratio: str

class AnalysisStatus(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    progress: int  # 0-100
    stage: str = ""
    result: dict = None
    error: str = None

def run_analysis_task(task_id: str, asset_ids: List[str], target_duration: int, db: Session):
    """åå°ä»»åŠ¡ï¼šæ‰§è¡Œè§†é¢‘åˆ†æ"""
    try:
        analysis_tasks[task_id]["status"] = "running"

        results = []

        for i, asset_id in enumerate(asset_ids):
            asset = db.query(Asset).filter(Asset.id == asset_id).first()
            if not asset:
                continue

            # è¿›åº¦å›è°ƒ
            def progress_callback(stage: str, progress: int):
                overall_progress = int((i / len(asset_ids) + progress / 100 / len(asset_ids)) * 100)
                analysis_tasks[task_id]["progress"] = overall_progress
                analysis_tasks[task_id]["stage"] = f"åˆ†æç´ æ {i+1}/{len(asset_ids)}: {stage}"

            # åˆ†æè§†é¢‘
            result = analysis_service.analyze_video(
                asset.file_path,
                target_duration,
                progress_callback
            )

            results.append(result)

            # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            asset.analyzed = True
            asset.analysis_result = json.dumps(result)
            db.commit()

        # ç”Ÿæˆå‰ªè¾‘å»ºè®®
        suggestions = analysis_service.generate_edit_suggestions(results, target_duration)

        analysis_tasks[task_id]["status"] = "completed"
        analysis_tasks[task_id]["progress"] = 100
        analysis_tasks[task_id]["result"] = {
            "analysis_results": results,
            "edit_suggestions": suggestions
        }

    except Exception as e:
        analysis_tasks[task_id]["status"] = "failed"
        analysis_tasks[task_id]["error"] = str(e)

@router.post("/start", response_model=AnalysisStatus)
async def start_analysis(
    request: StartAnalysisRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """å¯åŠ¨AIåˆ†æä»»åŠ¡"""
    # éªŒè¯ç´ æå­˜åœ¨
    for asset_id in request.asset_ids:
        asset = db.query(Asset).filter(Asset.id == asset_id).first()
        if not asset:
            raise HTTPException(status_code=404, detail=f"ç´ æ {asset_id} ä¸å­˜åœ¨")

    # åˆ›å»ºä»»åŠ¡
    task_id = str(uuid.uuid4())
    analysis_tasks[task_id] = {
        "status": "pending",
        "progress": 0,
        "stage": "å‡†å¤‡ä¸­",
        "result": None,
        "error": None
    }

    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(
        run_analysis_task,
        task_id,
        request.asset_ids,
        request.target_duration,
        db
    )

    return AnalysisStatus(
        task_id=task_id,
        status="pending",
        progress=0
    )

@router.get("/status/{task_id}", response_model=AnalysisStatus)
async def get_analysis_status(task_id: str):
    """æŸ¥è¯¢åˆ†æä»»åŠ¡çŠ¶æ€"""
    if task_id not in analysis_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    task = analysis_tasks[task_id]

    return AnalysisStatus(
        task_id=task_id,
        status=task["status"],
        progress=task["progress"],
        stage=task.get("stage", ""),
        result=task.get("result"),
        error=task.get("error")
    )
```

åœ¨ `backend/api/main.py` ä¸­æ³¨å†Œ:

```python
from api.routes import analysis

app.include_router(analysis.router)
```

**Step 4: æäº¤è§†é¢‘åˆ†ææœåŠ¡**

```bash
git add backend/services/video_analysis_service.py backend/api/routes/analysis.py
git commit -m "feat(backend): add complete video analysis service

- Create VideoAnalysisService integrating AI services
- Implement full analysis pipeline (frames + audio)
- Add edit suggestion generation algorithm
- Create analysis API with background tasks
- Track analysis progress and status

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## å‰ç«¯AIåŠŸèƒ½

### Task 4: AIåˆ†æUI

**Files:**
- Create: `frontend/src/components/AIAnalysis/AnalysisDialog.tsx`
- Create: `frontend/src/components/AIAnalysis/ProgressBar.tsx`
- Create: `frontend/src/components/AIAnalysis/ResultPanel.tsx`
- Create: `frontend/src/services/analysisApi.ts`

**Step 1: åˆ›å»ºåˆ†æAPIå®¢æˆ·ç«¯**

åœ¨ `frontend/src/services/analysisApi.ts`:

```typescript
import apiClient from './api';

export interface StartAnalysisRequest {
  asset_ids: string[];
  target_duration: number;
  aspect_ratio: string;
}

export interface AnalysisStatus {
  task_id: string;
  status: 'pending' | 'running' | 'completed' | 'failed';
  progress: number;
  stage?: string;
  result?: any;
  error?: string;
}

export const analysisApi = {
  /**
   * å¯åŠ¨åˆ†æä»»åŠ¡
   */
  async startAnalysis(request: StartAnalysisRequest): Promise<AnalysisStatus> {
    const response = await apiClient.client.post('/api/analysis/start', request);
    return response.data;
  },

  /**
   * æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
   */
  async getStatus(taskId: string): Promise<AnalysisStatus> {
    const response = await apiClient.client.get(`/api/analysis/status/${taskId}`);
    return response.data;
  },

  /**
   * è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ
   */
  async pollStatus(
    taskId: string,
    onProgress: (status: AnalysisStatus) => void,
    interval: number = 2000
  ): Promise<AnalysisStatus> {
    return new Promise((resolve, reject) => {
      const poll = async () => {
        try {
          const status = await this.getStatus(taskId);
          onProgress(status);

          if (status.status === 'completed') {
            resolve(status);
          } else if (status.status === 'failed') {
            reject(new Error(status.error || 'åˆ†æå¤±è´¥'));
          } else {
            setTimeout(poll, interval);
          }
        } catch (error) {
          reject(error);
        }
      };

      poll();
    });
  }
};
```

**Step 2: åˆ›å»ºè¿›åº¦æ¡ç»„ä»¶**

åœ¨ `frontend/src/components/AIAnalysis/ProgressBar.tsx`:

```tsx
import React from 'react';
import './ProgressBar.css';

interface ProgressBarProps {
  progress: number;
  stage?: string;
}

const ProgressBar: React.FC<ProgressBarProps> = ({ progress, stage }) => {
  return (
    <div className="progress-container">
      <div className="progress-bar">
        <div
          className="progress-fill"
          style={{ width: `${progress}%` }}
        />
      </div>
      <div className="progress-info">
        <span className="progress-stage">{stage || 'å¤„ç†ä¸­...'}</span>
        <span className="progress-percent">{progress}%</span>
      </div>
    </div>
  );
};

export default ProgressBar;
```

åœ¨ `frontend/src/components/AIAnalysis/ProgressBar.css`:

```css
.progress-container {
  width: 100%;
}

.progress-bar {
  width: 100%;
  height: 8px;
  background-color: #444;
  border-radius: 4px;
  overflow: hidden;
  margin-bottom: 12px;
}

.progress-fill {
  height: 100%;
  background: linear-gradient(90deg, #1890ff, #40a9ff);
  transition: width 0.3s ease;
}

.progress-info {
  display: flex;
  justify-content: space-between;
  font-size: 14px;
}

.progress-stage {
  color: #ccc;
}

.progress-percent {
  color: #1890ff;
  font-weight: 500;
}
```

**Step 3: åˆ›å»ºåˆ†æå¯¹è¯æ¡†**

åœ¨ `frontend/src/components/AIAnalysis/AnalysisDialog.tsx`:

```tsx
import React, { useState } from 'react';
import { analysisApi } from '@/services/analysisApi';
import ProgressBar from './ProgressBar';
import './AnalysisDialog.css';

interface AnalysisDialogProps {
  assetIds: string[];
  onClose: () => void;
  onComplete: (result: any) => void;
}

const AnalysisDialog: React.FC<AnalysisDialogProps> = ({
  assetIds,
  onClose,
  onComplete
}) => {
  const [step, setStep] = useState<'config' | 'analyzing' | 'result'>('config');
  const [targetDuration, setTargetDuration] = useState(180); // 3åˆ†é’Ÿ
  const [aspectRatio, setAspectRatio] = useState('9:16');
  const [progress, setProgress] = useState(0);
  const [stage, setStage] = useState('');
  const [error, setError] = useState('');

  const handleStart = async () => {
    setStep('analyzing');
    setError('');

    try {
      const response = await analysisApi.startAnalysis({
        asset_ids: assetIds,
        target_duration: targetDuration,
        aspect_ratio: aspectRatio
      });

      // è½®è¯¢çŠ¶æ€
      const finalStatus = await analysisApi.pollStatus(
        response.task_id,
        (status) => {
          setProgress(status.progress);
          setStage(status.stage || '');
        }
      );

      setStep('result');
      onComplete(finalStatus.result);

    } catch (err) {
      setError((err as Error).message);
      setStep('config');
    }
  };

  return (
    <div className="analysis-dialog-overlay">
      <div className="analysis-dialog">
        <div className="dialog-header">
          <h2>AIè‡ªåŠ¨åˆ†æ</h2>
          {step === 'config' && (
            <button onClick={onClose} className="close-button">âœ•</button>
          )}
        </div>

        <div className="dialog-content">
          {step === 'config' && (
            <>
              <div className="form-group">
                <label>ç›®æ ‡æ—¶é•¿</label>
                <input
                  type="range"
                  min={60}
                  max={600}
                  step={30}
                  value={targetDuration}
                  onChange={(e) => setTargetDuration(Number(e.target.value))}
                />
                <div className="range-labels">
                  <span>1åˆ†é’Ÿ</span>
                  <span className="range-value">
                    {Math.floor(targetDuration / 60)}åˆ†{targetDuration % 60}ç§’
                  </span>
                  <span>10åˆ†é’Ÿ</span>
                </div>
              </div>

              <div className="form-group">
                <label>ç”»å¹…æ¯”ä¾‹</label>
                <div className="radio-group">
                  <label>
                    <input
                      type="radio"
                      value="9:16"
                      checked={aspectRatio === '9:16'}
                      onChange={(e) => setAspectRatio(e.target.value)}
                    />
                    9:16 ç«–å±ï¼ˆæ¨èï¼‰
                  </label>
                  <label>
                    <input
                      type="radio"
                      value="16:9"
                      checked={aspectRatio === '16:9'}
                      onChange={(e) => setAspectRatio(e.target.value)}
                    />
                    16:9 æ¨ªå±
                  </label>
                  <label>
                    <input
                      type="radio"
                      value="1:1"
                      checked={aspectRatio === '1:1'}
                      onChange={(e) => setAspectRatio(e.target.value)}
                    />
                    1:1 æ–¹å½¢
                  </label>
                </div>
              </div>

              {error && (
                <div className="error-message">{error}</div>
              )}

              <div className="dialog-actions">
                <button onClick={onClose} className="btn-secondary">
                  å–æ¶ˆ
                </button>
                <button onClick={handleStart} className="btn-primary">
                  å¼€å§‹åˆ†æ
                </button>
              </div>
            </>
          )}

          {step === 'analyzing' && (
            <div className="analyzing-state">
              <div className="spinner" />
              <h3>AIåˆ†æä¸­...</h3>
              <p>æ­£åœ¨ç†è§£è§†é¢‘å†…å®¹ï¼Œè¯·ç¨å€™</p>
              <ProgressBar progress={progress} stage={stage} />
              <p className="hint">é¢„è®¡éœ€è¦ 3-5 åˆ†é’Ÿ</p>
            </div>
          )}

          {step === 'result' && (
            <div className="result-state">
              <div className="success-icon">âœ“</div>
              <h3>åˆ†æå®Œæˆï¼</h3>
              <p>å·²ç”Ÿæˆå‰ªè¾‘å»ºè®®</p>
              <button onClick={onClose} className="btn-primary">
                æŸ¥çœ‹ç»“æœ
              </button>
            </div>
          )}
        </div>
      </div>
    </div>
  );
};

export default AnalysisDialog;
```

åœ¨ `frontend/src/components/AIAnalysis/AnalysisDialog.css`:

```css
.analysis-dialog-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.7);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.analysis-dialog {
  background-color: #353535;
  border-radius: 8px;
  width: 500px;
  max-width: 90%;
  box-shadow: 0 8px 24px rgba(0, 0, 0, 0.5);
}

.dialog-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 20px 24px;
  border-bottom: 1px solid #444;
}

.dialog-header h2 {
  font-size: 20px;
  color: #fff;
  margin: 0;
}

.close-button {
  background: none;
  border: none;
  color: #888;
  font-size: 24px;
  cursor: pointer;
  padding: 0;
  width: 32px;
  height: 32px;
}

.close-button:hover {
  color: #fff;
}

.dialog-content {
  padding: 24px;
}

.form-group {
  margin-bottom: 24px;
}

.form-group label {
  display: block;
  font-size: 14px;
  color: #ccc;
  margin-bottom: 8px;
}

.form-group input[type="range"] {
  width: 100%;
}

.range-labels {
  display: flex;
  justify-content: space-between;
  font-size: 12px;
  color: #888;
  margin-top: 8px;
}

.range-value {
  color: #1890ff;
  font-weight: 500;
}

.radio-group {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.radio-group label {
  display: flex;
  align-items: center;
  gap: 8px;
  color: #fff;
  cursor: pointer;
  font-size: 14px;
}

.dialog-actions {
  display: flex;
  justify-content: flex-end;
  gap: 12px;
  margin-top: 24px;
}

.btn-primary,
.btn-secondary {
  padding: 10px 20px;
  border-radius: 4px;
  font-size: 14px;
  cursor: pointer;
  border: none;
  transition: background-color 0.2s;
}

.btn-primary {
  background-color: #1890ff;
  color: white;
}

.btn-primary:hover {
  background-color: #40a9ff;
}

.btn-secondary {
  background-color: #444;
  color: #fff;
}

.btn-secondary:hover {
  background-color: #555;
}

.analyzing-state,
.result-state {
  text-align: center;
  padding: 40px 20px;
}

.spinner {
  width: 48px;
  height: 48px;
  border: 4px solid #444;
  border-top-color: #1890ff;
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin: 0 auto 24px;
}

@keyframes spin {
  to { transform: rotate(360deg); }
}

.analyzing-state h3,
.result-state h3 {
  font-size: 20px;
  color: #fff;
  margin: 0 0 8px;
}

.analyzing-state p,
.result-state p {
  color: #ccc;
  margin: 0 0 24px;
}

.hint {
  font-size: 12px;
  color: #888;
}

.success-icon {
  width: 64px;
  height: 64px;
  border-radius: 50%;
  background-color: #52c41a;
  color: white;
  font-size: 36px;
  display: flex;
  justify-content: center;
  align-items: center;
  margin: 0 auto 24px;
}

.error-message {
  padding: 12px;
  background-color: #ff4d4f22;
  border: 1px solid #ff4d4f;
  border-radius: 4px;
  color: #ff7875;
  font-size: 14px;
  margin-top: 16px;
}
```

**Step 4: æäº¤AIåˆ†æUI**

```bash
git add frontend/src/components/AIAnalysis/ frontend/src/services/analysisApi.ts
git commit -m "feat(frontend): add AI analysis UI components

- Create AnalysisDialog with config and progress
- Implement progress bar with stages
- Add polling mechanism for task status
- Style with modern dialog design

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## éªŒæ”¶æ ‡å‡†

**Phase 3å®Œæˆæ ‡å‡†ï¼š**

- [x] é€šä¹‰åƒé—®SDKé›†æˆ
- [x] è®¯é£æ˜Ÿç«SDKé›†æˆï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
- [x] è§†é¢‘åˆ†ææœåŠ¡å®Œæ•´æµç¨‹
- [x] AIåˆ†æAPIå’Œåå°ä»»åŠ¡
- [x] AIåˆ†æUIå¯¹è¯æ¡†
- [x] è¿›åº¦æ˜¾ç¤ºå’Œè½®è¯¢

**é¢„è®¡æ€»æ—¶é—´: 24-30å°æ—¶ï¼ˆ3å‘¨ï¼‰**

---

## é…ç½®æé†’

**å¼€å‘å‰å¿…é¡»é…ç½®ï¼š**

1. **é€šä¹‰åƒé—®API Key**
   - åœ¨ `backend/config.py` æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½® `QWEN_API_KEY`
   - è·å–åœ°å€: https://dashscope.aliyun.com/

2. **è®¯é£æ˜Ÿç«API**
   - åœ¨ `backend/config.py` ä¸­è®¾ç½® `XUNFEI_APPID`, `XUNFEI_API_KEY`, `XUNFEI_API_SECRET`
   - è·å–åœ°å€: https://www.xfyun.cn/
   - æ³¨æ„ï¼šå½“å‰å®ç°ä¸ºæ¨¡æ‹Ÿç‰ˆï¼Œéœ€è¦å®Œæ•´å®ç°WebSocketè¿æ¥

---

## ä¸‹ä¸€æ­¥

Phase 3å®Œæˆåè¿›å…¥Phase 4: å®Œå–„ä¸ä¼˜åŒ–
